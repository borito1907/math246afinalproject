\documentclass{article}
\usepackage{geometry}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath, amsthm, amssymb}
\usepackage{parskip}
\usepackage{xr}
\usepackage{textcomp}
\usepackage{hyperref}

\newgeometry{vmargin={15mm}, hmargin={24mm,34mm}}
\theoremstyle{definition} 
\newtheorem{definition}{Definition}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}

\newcommand{\mytilde}{\raisebox{0.5ex}{\texttildelow}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\logderivative}[1]{\frac{#1'}{#1}}
\newcommand{\riemannsphere}{\C \cup \{ \infty \}}

\title{Prime Number Theorem}
\author{Boran Erol}

\begin{document}

\maketitle

\section{Introduction}

The Prime Number Theorem is one of the most celebrated theorems in mathematics. In this writeup,
we look at some of the history of the Prime Number Theorem and review Newman's proof with a little bit 
more detail and references to our class this quarter. At the end,
I will give a brief overview of Alan Turing's relationship to the Riemann Hypothesis
and point out an interesting connection to a remark made in class this quarter.

Let $ \pi(x) $ be the function that counts the primes up to $ x $, i.e.\[ \pi(x) := \text{number of primes less than or equal to $ x $} .\]

The Prime Number Theorem states that $ \pi(x) \sim \frac{x}{\log x} $, where $\log $ denotes the natural logarithm.
Here, $ \sim $ means that \[ \lim_{x \to \infty} \frac{ \pi(x)}{x/ \log{x}} = 1 \]

In other words, the Prime Number Theorem states that the $n$th prime will have size roughly $ n \log n $.
In fact, Barkley Rosser proved in 1939 that the nth prime number is strictly greater than $ n \log n $ \cite{rosser_n-th_1939}.

The Prime Number Theorem was first conjectured by Gauss when he was around 15 (in fact, 
he conjectured that $ \pi(x) \sim Li(x) $, where $ Li(x) $ is the \href{https://en.wikipedia.org/wiki/Logarithmic_integral_function}{logarithmic integral function}, which turns out to be a better approximation) 
using prime number tables compiled by Johann Heinrich Lambert, who is
known for proving that $ \pi $ is irrational \cite{klyve_origin_2018} \cite{noauthor_histoire_nodate}. Legendre, seemingly independently from Gauss,
guessed in 1808 that

\[ \pi(x) \sim \frac{x}{\log x - A(x)} \]

for some $ A(x) $ that tends to a constant as $ x $ goes to infinity \cite{bateman_hundred_1996}. In fact,
earlier in his writing, Legendre even specifies this constant, stating 

\[ \pi(x) \sim \frac{x}{\log x - 1.08366} \]

instead \cite{bambah_centennial_2000}, but no explanation for this constant has been found in Legendre's notes.
Following Gauss and Legendre, Chebyshev came along and proved that $ \pi(x) = \Theta(x) $,
and provided extremely good constants. More specifically, Chebyshev \cite{bateman_hundred_1996} proved that for 
sufficiently large $ x $ we have that

\[ 0.92 \frac{x}{\log x} \leq \pi(x) \leq 1.10 \frac{x}{\log x}. \]

In fact, Newman's short proof \cite{zagier_newmans_1997} still uses Chebyshev's upper bound
(or at least the idea behind the upper bound). We'll skip the proof of the upper bound, because
it doesn't relate to complex analysis and also because it isn't the interesting portion of the proof.
You can see the upper bound in virtually every reference mentioned in this writeup, or \href{https://artofproblemsolving.com/wiki/index.php/Chebyshev_theta_function#Estimates_of_the_function}
{this AoPS page}.

Moreover, Chebyshev's arguments use elementary tricks using the binomial coefficients,
so the `hardness' of the Prime Number Theorem is proving that the $ \pi(x) \sim \frac{x}{\log x} $,
not $ \pi(x) = \Theta(\frac{x}{\log x}) $.

In 1859, Riemann came along and published the paper that introduces the
Riemann Hypothesis in an 8-page paper \cite{wilkins_number_1998}. In the paper, Riemann also
mentions the conjecture that $ \pi(x) < Li(x) $ (though it's not certain whether he believed it), which would later
be disproved by Littlewood and capture Alan Turing's interest \cite{princetonacademics_andrew_2012} \cite{matiyasevich_riemann_2020} \cite{hejhal_alan_2012}. More importantly,
Riemann considers the Zeta function first introduced by Euler in 1737 
with complex inputs and proves that proving the Prime Number Theorem
reduces to showing that the Zeta function doesn't have any zeros with $ Re(s) = 1 $.

In 1896, approximately a 100 year after Gauss's conjecture, Hadamard and de la Vallée Poussin
independently proved the Prime Number Theorem using Riemann's proposed method. De la Vallée Poussin's
initial solution was apparently incredibly messy, and he also admits that his solution was worse than
Hadamard's \cite{bambah_centennial_2000}. Over the next 100 years, the proofs would be improved with new tricks and observations, 
and in 1980 Newman discovered the short proof that we present in this writeup.

In 1949, Selberg and Erdös (there's a priority dispute, and Richard Borcherds thinks
that Selberg wins this dispute since Erdös used Selberg's identity to prove the result, which is the
key ingredient, but this is obviously a subjective matter) proved the Prime Number Theorem using non-analytic methods \cite{bambah_centennial_2000} \cite{richard_e_borcherds_introduction_2022}.

\subsection{Overview}

Let's now introduce some notation. Throughout, $ s $ will denote a complex number with $ s = \sigma + it $ and $ \sigma, t \in \mathbb{R} $.
Moreover, $ p $ will denote a prime and $ \rho $ will denote a zero of $ \zeta(s) $.
When we use the notation 

\[ \sum_{p}, \sum_{\rho}\]

this means summing (or multiplying) over all primes or the zeros of $ \zeta $, respectively. We'll introduce
the rest of the notation as needed to not bore the reader.

There are many resources online that try to explain the Prime Number Theorem and many posts
on MSE that try to clarify the proof \cite{bandeira_complex_nodate} \cite{avi_why_2015} \cite{coffee_table_what_2018}. 
As I was exploring these resources myself to try to
understand the Prime Number Theorem, I realized that (like me) most beginners
had trouble understanding the so-called `Analytic Theorem' in Newman's paper.

Thus, after defining the Zeta function and proving Euler's Identity, I'll
make some handwavy arguments relating the Analytic Theorem in Newman's paper
to the Ikehara-Wiener Theorem, and see how the Analytic Theorem in Newman's paper
is a simplification of this theorem for which we can develop some intuition. I hope these
arguments will help other people studying the Prime Number Theorem for the first time,
since I think it would have helped me.

After this point, the proof will be done modulo proving certain properties of the Zeta function.
Thus, with this motivation, I will then prove that the Zeta function can be extended to the
right-half of the complex plane and also prove that it doesn't have any zeros with $ Re(s) = 1 $.

\section{The Basic Objects and Their Properties}

Let's now define the star of the show, the Zeta function and
try to understand how it relates to prime numbers.

Let \[ \zeta(s) := \sum_{n = 1}^{\infty} \frac{1}{n^{s}} .\] 

Notice that for $ s $ with $ Re(s) > 1 $ we have that the infinite series is absolutely
summable and therefore convergent. This is because if the infinite series is absolutely
summable, then the real and imaginary parts are absolutely summable, which implies that
the real and imaginary parts of the series converge, which implies that the complex-valued
sequence of partial sums converges. Also notice that $ \zeta(s) $ has a simple pole
at $ s = 1 $ since the harmonic series diverges. 

Let us now immediately relate this function to primes by proving Euler's Identity:

\begin{lemma}
    For all $ s \in \C $ such that $ Re(s) > 1 $, we have that $ \zeta(s) = \prod_{p} \frac{1}{1 - p^{-s}} $.
\end{lemma}

\begin{proof}
    Let $ s = \sigma + it $ with $ \sigma > 1 $.
    Then, the geometric series defined by $ \frac{1}{1 - p^{-s}} $ is absolutely convergent by the
    same argument as the convergence of the Zeta function.

    Now, notice that

    \[ \lvert \zeta(s) - \prod_{p \leq q} \frac{1}{1 - p^{-s}} \rvert < \sum_{n = q + 1}^{\sum} \frac{1}{n^{\sigma}} \]

    since every $ q $ is a product of primes each of which is less than $ q $.

    Taking $ q \to \infty $ produces the desired result.
\end{proof}

Notice that the product formula also implies that $ \zeta(s) \neq 0 $ if $ Re(s) > 1 $ since
none of the terms in the infinite product is zero.

Let us consider the logarithmic derivative of $ \zeta(s) $.
Recall that in class, we mentioned that the log derivative of a finite product is the sum of the
log derivative of the individual terms. This can in fact be extended to infinite products
by considering infinite product as infinite sums of logarithms. Taking this for granted,
let's now see what the derivative of a single term looks like. Notice that


\[ \frac{d}{ds} (1 - p^{-s})^{-1} = - (1 - p^{-s})^{-2} \log(p) p^{-s}\]

Thus, 

\[ \frac{\frac{d}{ds} (1 - p^{-s})^{-1}}{(1 - p^{-s})^{-1}} = \frac{\log(p) p^{-s}}{1 - p^{-s}} = - \frac{\log (p)}{p^{s} - 1} \]

Then, 

\[ \frac{\frac{d}{ds} \zeta(s)}{\zeta(s)} = - \sum_{p} \frac{\log (p)}{p^{s} - 1} \]


Now, I'll first wave my hands for two paragraphs and give an intuitive argument
using the residue theorem, and then I'll compare this argument to a rigorous
but super counterintuitive proof.

Notice that the right-hand side of this equation is counting the number of primes, albeit
weighted a in a little bit of a weird way. However, we can also see the right-hand side the as
`essentially' the following infinite series (notice that I've already starting waving my hands
by omitting the $ -1 $ from the denominator):

\[ \Phi(s) = \sum_{n} \frac{\Lambda(n)}{n^{s}}, \]

where $ \Lambda(n) = \log p $ if $ n $ is the power of a prime number and $ 0 $ otherwise.

This is called a \textbf{Dirichlet series} and $ \Lambda(n) $ is called the \textbf{von Mangoldt function}.

It turns out that in 1896, Mellin figured out how to get back the growth of a Dirichlet series
in terms of its transform, the so called Mellin transform. Then, Wiener and Ikenara
took this idea and proved the following theorem in 1932 \cite{korevaar_newmans_1982}:

\begin{theorem}
    Let $ f(x) $ be non-negative and nondecreasing on $ [1,\infty) $ such that the Merlin
    transform

    \[ g_{0}(s) = - f(1) + s \int_{1}^{\infty} f(x) x^{-s - 1} dx \]
    
    exists for $ Re(s) > 1 $. Suppose that for some constant $ c $, the function 
    \[ g_{0}(s) - \frac{c}{s-1} \]

    has a continuous extension to the closed half-plane $ Re(s) \geq 1 $. Then, $ \frac{f(x)}{x} \to 1 $
    as $ x \to \infty $.
\end{theorem}

I'm just putting this statement here for intuition, so I'm not going to explain this statement in detail.
All I want to say is that it intuitively makes sense that $ \Phi(s) $ will play the role of $ g(s) $,
$ \Lambda(x) $ will play the role of $ f(x) $, and $ c $ will turn out to be $ 1 $.

We'll prove an easier version of this statement (\cite{korevaar_newmans_1982} calls it `A Poor Man's Ikehara-Wiener Theorem')
which is sufficient to prove the Prime Number Theorem.

But now, notice that in order to use this theorem, we need to show that $ \Phi(s) - \frac{1}{s - 1} $
can be continued analytically on the real line. This is where the crucial observation of Riemann
comes in. Showing that $ \Phi(s) - \frac{1}{s - 1} $ is analytic on $ Re(s) \geq 1 $, using the Residue
Theorem, is equivalent to proving that $ \zeta(s) \neq 0 $ for any $ s $ with $ Re(s) = 1 $ (assuming,
of course, that $ \zeta $ doesn't have any poles, which is relatively easy to prove).
Riemann stated this in his 1859 paper, but couldn't prove this result.

Here's the statement we'll prove instead of the Ikehara-Wiener Theorem:

\begin{lemma}
    Let $ f:[0,\infty) \xrightarrow{} \C$ be a bounded and locally integrable function.
    Define 

    \[ g(z) := \int_{0}^{\infty} f(t) e^{-tz} dt, Re(z) > 0 \]

    Assume $ g(z) $ extends holomorphically to $ Re(z) \geq 0 $.
    Then, $ \int_{0}^{\infty} f(t) dt $ exists and equals to $ g(0) $.
\end{lemma}

To not break up the flow of the argument, I'll defer the proof of this statement to the appendix.
However, notice the similarities between this theorem and the Ikehara-Wiener Theorem.
In both theorems, we understand the asymptotic behavior of $ f(x) $ by using the
niceness properties of a Fourier-like integral. In fact, in our proof, we won't even need
the value $ g(0) $: convergence will be sufficient to deduce the Prime Number Theorem.

Let's now start the proof.

\section{Overview of the Proof}

Let $ \nu(x) $, which is sometimes called the \textbf{Chebyshev's theta function},
be defined as 

\[ \nu(x) = \sum_{p \leq x} \log p \]

Intuitively, this function
counts each prime and weights them according to the inverse of their density.
Thus, we expect $ \nu(x) \sim x $ if we believe the Prime Number Theorem.
In fact, this turns out to imply the Prime Number Theorem. The proof of this
lemma is pretty clear and is almost identical to Lemma \ref{lem:epsilon_convergence_trick_phi},
and also stated explicitly in Newman's paper unlike Lemma \ref{lem:epsilon_convergence_trick_phi},
so I omit it.

\begin{lemma}
    If $ \nu(x) \sim x $, $ \pi(x) \sim x \log x $.
\end{lemma}

Thus, we have reduced the Prime Number Theorem to proving $ \nu(x) \sim x $.

In order to do this, we'll show that the integral 

\[ \int_{1}^{\infty} \frac{\nu(x) - x}{x^{2}} \]

converges. Let's see why this would suffice intuitively. By Chebyshev's upper bound,
we know that $ \nu(x) = O(x) $. Now, notice that if $ \nu(x) $ wasn't exactly approaching $ x $,
the above integral would turn into the harmonic series and would blow up to infinity. Thus, $ \nu(x) $ should 
\textbf{exactly} cancel with $ x $. 

In order to prove that this integral is convergent, we'll use the Poor Man's Ikehara-Wiener Theorem.
More specifically, we'll pick $ f(t) := \nu(e^{t}) e^{-t} -1 $ and 
\[ g(z) = \frac{\Phi(z+1)}{z+1} - \frac{1}{z} \]

Well, these look like they have absolutely nothing to do with $ \frac{\nu(x) - x}{x^{2}} $,
but some trickery with integrals shows that they are actually equivalent. This
is done in $ \ref{lem:newman_v} $.

To apply the theorem, we'll need $ f(t) $ to be locally bounded 
and $ g(z) $ to be analytic on $ Re(z) \geq 0 $.
The fact that $ f(t) $ is bounded follows from the fact a $ \nu(x) = O(x) $. The analyticity of $ g(z) $ is equivalent to proving
that the Zeta function has no zeros with $ Re(s) = 1 $ by the calculus of residues. 
The formal proof is given in \ref{lem:newman_v_conditions}.

Thus, we have reduced the proof of the Prime Number Theorem to holomorphically
extending $ \zeta(s) - \frac{1}{s - 1} $ to the right-half plane and showing that it doesn't
have any zeros with $ Re(s) = 1 $. We now prove these to establish the Prime Number Theorem.

\section{Properties of the Zeta Function}

We'll now prove the following lemma:

\begin{lemma}
    $ \zeta(s) - \frac{1}{s - 1} $ extends holomorphically to $ Re(s) > 0 $.
\end{lemma}

It looks like there are `better' ways to derive this by using the functional equation
for $ \zeta $, but this is outside of the scope of the writeup.

This states that the Zeta function doesn't have any poles in the complex plane
other than $ s = 1 $.

The key idea in the proof is to realize that $ \frac{1}{s - 1} $
can be made to look like the Zeta function. More specifically,

\[ \frac{1}{s-1} = \int_{1}^{\infty} \frac{1}{x^{s}} dx\]
Then, we'll use the cancellation between $ \frac{1}{n^{s}} $ and
$ \frac{1}{x^{s}} $ to slow down the rate of divergence by a full power.
Here's the proof:

\begin{proof}
    We have that
    \begin{align*}
        \zeta(s) - \frac{1}{s - 1} = \sum_{n = 1}^{\infty} \frac{1}{n^{s}} - (\int_{1}^{\infty} \frac{1}{x^{s}} dx) \\
        = \sum_{n = 1}^{\infty} (\frac{1}{n^{s}} - \int_{n}^{n + 1} \frac{1}{x^{s}} dx) = \sum_{n = 1}^{\infty} \int_{n}^{n+1} (\frac{1}{n^{s}} - \frac{1}{x^{s}}) dx
    \end{align*}

    Notice that

    \begin{align*}
        \lvert \int_{n}^{n+1} (\frac{1}{n^{s}} - \frac{1}{x^{s}}) dx \rvert = \lvert \int_{n}^{n+1} (\int_{n}^{x} \frac{s}{u^{s+1}} du) dx \rvert \\
        \leq \sup_{u \in [n,n+1]} \frac{\lvert s \rvert}{\lvert u^{s+1} \rvert} = \frac{\lvert s \rvert}{n^{Re(s) + 1}}
    \end{align*} 

    Thus, 

    \[ \lvert \zeta(s) - \frac{1}{s - 1} \rvert \leq \sum_{n = 1}^{\infty} \frac{\lvert s \rvert}{n^{Re(s) + 1}} \]

    Since the right-side is convergent for any $ s $ with $ Re(s) > 0 $ and because
    the uniform limit of analytic functions is analytic, $ \zeta(s) $ is analytic on $ Re(s) > 0 $.
\end{proof}

We will now show that the Zeta function doesn't have any zeros with $ Re(s) = 1 $.

The proof uses a trick combined with the calculus of residues. Notice that

\begin{align}\label{log_derivative_zeta_and_phi}
    \frac{\zeta(s)^{\prime}}{\zeta(s)} - \Phi(s) = \sum_{p} \log(p) (\frac{1}{p^{s} - 1} - \frac{1}{p^{s}}) 
\end{align}
    
Rearranging, we get

\[ \frac{\zeta(s)^{\prime}}{\zeta(s)} = \Phi(s) + \sum_{p} \log(p)[\frac{1}{p^{s}(p^{s} - 1)}] \]

Notice that the infinite sum on the right hand-side in \ref{log_derivative_zeta_and_phi} converges for $ Re(s) > 1/2 $,
which is proved rigorously in \ref{lem:epsilon_convergence_trick_phi} but is easy 
to see by noticing that it's essentially equivalent to summing terms that look like
$ n^{-2s} $.

Thus, we can control the zeros of the Zeta function using $ \Phi(s) $. We know that $ (p^{i \alpha} + p^{-i \alpha})^{2k} $
is a real number for any integer $ k $. Notice that such expressions appear in the denominator
of $ \Phi(s) $. We now use this combined with the simple pole of the Zeta function to get
an equation relating the order of the zero at $ 1, 1 \pm i \alpha $ and $ 1 \pm 2i \alpha $.

By contradiction, assume that $ s = 1 + i \alpha $ is a zero of order $ \mu > 0 $
for some $ \alpha > 0 $. Similarly, let $ \nu $ be the `order' of the zero at 
$ s = 1 + 2i \alpha $. Notice that $ \nu $ might be zero, which would imply
that $ \zeta(1 + 2 \alpha i) \neq 0 $, which is fine. Also notice that $ \bar{\zeta(s)} = \zeta(\bar(s)) $.
Thus, $ 1 - i \alpha $ is also a zero of order $ \mu $ and similarly for $ 1 - 2 i \alpha $.

Now, recall from lecture that the residue at a simple pole $ z_{0} $
might be computed by 

\[ \lim_{z \to z_{0}} (z - z_{0})f(z), \]

which we proved by considering the Laurent series. By applying this idea to $ \Phi $ 
(which is equivalent to applying it to the log derivative since the sum 
on the right-hand side in \ref{log_derivative_zeta_and_phi} is absolutely convergent, so it vanishes as we take $ \epsilon $ to 0), we get the following equalities

\[ \lim_{\epsilon \to 0^{+}} \epsilon  \frac{-\zeta(1 + \epsilon)^{\prime}}{\zeta(1 + \epsilon)}  = \lim_{\epsilon \to 0^{+}}  \epsilon \Phi(1 + \epsilon) = 1 \]

\[ \lim_{\epsilon \to 0^{+}} \epsilon  \frac{-\zeta(1 + \epsilon \pm i\alpha)^{\prime}}{\zeta(1 + \epsilon \pm i\alpha)}  = \lim_{\epsilon \to 0^{+}} \epsilon \Phi(1 + \epsilon \pm i\alpha) = - \mu \]

\[ \lim_{\epsilon \to 0^{+}} \epsilon  \frac{-\zeta(1 + \epsilon \pm 2i\alpha)^{\prime}}{\zeta(1 + \epsilon \pm 2i\alpha)} = \lim_{\epsilon \to 0^{+}} \epsilon \Phi(1 + \epsilon \pm 2i\alpha) = - \nu \]

Now, we notice that 

\begin{align*}
    \sum_{r = - 2}^{2} \binom{4}{2 + r} \phi(1 + \epsilon + ir \alpha) = \sum_{r = - 2}^{2} \binom{4}{2 + r} \sum_{p} \frac{\log (p)}{p^{1 + \epsilon + ir \alpha}} \\
    \sum_{p} \frac{\log (p)}{p^{1 + \epsilon}} \sum_{r = - 2}^{2} \binom{4}{2 + r} p^{-ir \alpha} = 
    \sum_{p} \frac{\log p}{p^{1 + \epsilon}} (p^{\frac{i \alpha}{2}} + p^{\frac{- i \alpha}{2}})^{4}
\end{align*}

Noticing that the last expression is nonnegative, we get that $ 2 \nu + 8 \mu \leq 6 $.
But then, $ \mu = 0 $ and we have reached a contradiction.
Essentially, what we are saying is that the negative contribution of the zeros can't be greater
than the positive contribution of the pole, and that we can artificially increase
the contributions of $ \mu $ by considering a second zero. 
This proof gives me zero intuition for why this fact is true, however.

\section{The Riemann Hypothesis and Alan Turing}

After looking at these arguments, one can also gather why the Riemann Conjecture would
be useful in controlling the error term in the approximations: better understanding
of the zeros of the Zeta function would allow us to use similar methods in order approximate
the error term in the Prime Number Theorem. In particular, we have the following equality by 
Riemann \cite{princetonacademics_andrew_2012}:

\[ \pi(x) = Li(x) - \sum_{\rho} Li(x^{\rho}) + O(x^{1/2} \log x),\]

where $ Li(x) = \int_{2}^{\infty} \frac{dt}{\log t} $ is the we logarithmic integral function we mentioned earlier.
Thus, if all the zeros of the Zeta function are on the line $ Re(s) = 1/2 $,
we'd have that

\[ \pi(x) = Li(x) + O(x^{1/2} \log x) \]

This would, in some vague sense, tell us that the primes are distributed randomly and obey
the Central Limit Theorem with a variance of $ \sqrt{x} $. In fact, this randomness analogy goes much further and has a great story
where Hugh Montgomery and Freeman Dyson have tea, which I have chosen to ignore
completely since it's super popular.

I come from a computer science background, and therefore I was fascinated to learn
that Alan Turing also did some work on the Zeta function. In fact, one of
his contributions involved developing the so-called Turing method to aid in the
numerical verification of the positions of the zeros of the Zeta function
that obviates the use of the argument principle sketched during lecture.

Let's now recall the argument from lecture: use numerical methods to estimate the location
of a zero, compute the integral of the logarithmic derivative around the estimated position of the zero, 
and check whether the numerical integral gives a value close to $ 1 $. Even though numerical integration is imprecise,
since the winding number is an integer, we can be certain that the zero is on
the critical line if we can bound the imprecision of the numerical method.

Instead, Alan Turing proposed using the formula (that I don't really understand)

\[ N(t) = 1 + \frac{1}{\pi} \theta(t) + S(t) \]

where $N(t)$ counts the number of zeros in the critical strip with imaginary part 
in $ [0,t] $ and  $ \theta(t) $ is a smooth function called the Riemann-Siegel theta function.
Since $ N(t) $ isn't smooth and $ 1 + \frac{1}{\pi} \theta(t) $ is, $ S(t) $ should 
also have jump discontinuities to catch up with $ N(t) $. Moreover, if $ S(t) $ jumps 
by two at any point (unless the Zeta function has non-simple zeros, which, as stated in lecture,
would be super surprising for reasons beyond me), we would gather that the zero was not on the
critical line. I found this super interesting since it both pertained to something we talked about
in lecture and Alan Turing!

Another gem I found while looking into this is the following story: 
Alan Turing apparently got a 40 pound grant to create an electronic chip
specifically designed to compute the zeros of the Zeta function. However, after the
outbreak of World War II, he couldn't finish this project \cite{hejhal_alan_2012}.
However, he later returned to this problem in the 1950s and used a computer to 
compute zeros of the Zeta function. In fact, he was the first person in history to do so.
Also, he was the first person to compute zeros with high imaginary components, since he seems
to have believed that that's where a counterexample would be found.

Here's a quote from his notes that demonstrates his interest \cite{hejhal_alan_2012}:

`[i]f it had not been for the fact that the computer remained in serviceable condition for an unusually long period from 3 p.m. one afternoon to 8 a.m. the
 following morning it is probable that the calculations would never have been done at all.'

Thus, Alan Turing stayed up all night in order to compute the zeros of the Zeta function! From his 1953 paper,
testimonies of people that interacted with him, we also know that Alan Turing's skepticism for the Riemann Hypothesis
increased over time. Towards the end of his life, Alan Turing believed strongly that the Riemann Hypothesis was false,
which I also thought was very interesting.

\section{Conclusion}

The history, statement and the proof of the Prime Number Theorem is fascinating, but doing this project
also made me notice that sometimes simplified proofs that uses poor man's lemmas and cool tricks isn't that useful for understanding.
I found that I didn't gain that much satisfaction from understanding this proof, even though it was one of the
things that excited me the most about taking this class. I'd say that the main benefit
of trying to understand Newman's proof of the Prime Number Theorem is that the unsatisfaction that arises out of 
the proof made me want to study concepts such as the actual Ikehara-Wiener theorem and understand the result in a
deeper way.

\newpage

\bibliographystyle{plainurl} 
\bibliography{246a} 

\newpage

\appendix


\section{A Poor Man's Ikehara-Wiener Theorem}

Assume that $ f(t) $ is bounded by some $ M > 0 $.

For every $ T > 0 $, let $ F_{T}(s) := \int_{0}^{T} f(t)e^{-st}dt $.
By Lemma 5.4 in \cite{shakarchi_complex_nodate}, $ F_{T}(s) $ is holomorphic on the entire complex plane.

Our goal is to show that 

\[ \lim_{T \to \infty} F_{T}(0) = F(0) \] 


The idea behind the proof is to use tricks with the Cauchy Integral Formula
in order to bound the difference between $ F(s) $ and $ F_{T}(s) $.

Let $ R > 0 $.
Consider the semicircle of radius $ R $ centered at the origin in the right half-plane.
Since this set is compact and $ F(s) $ admits an analytic extension on an open 
set containing the semicircle, there
exists some $ \delta_{R} > 0 $ such that $ F(s) $ is analytic on 

\[ z \in \C : Re(s) \geq \delta, \lvert s \rvert \leq R \]

Let $ \gamma_{R} $ be the curve that traces the boundary of the semicircle in the right half plane,
keeps going until it hits the $ Re(s) = - \delta_{R} $ level, and then travels down and back along
the circle. The Newman's Tauberian Theorem page in AoPS has a good illustration (LINK). Let $ \gamma_{R}^{+} $
be the portion of the curve that is in the right half-plane and $ \gamma_{R}^{-} $ be the portion in the
left half-plane.

Before seeing Newman's trick, let's first see why this trick is necessary.

By Cauchy's Integral Formula (since $ F(s) $ and $ F_{T}(s) $ are holomorphic inside the curve),
we have that 

\[ F(0) - F_{T}(0) = \frac{1}{2 \pi i} \int_{\gamma_{R}} [F(s) - F_{T}(s)] \frac{1}{s} ds \]

Now, notice that our prospects seem bleak. For $ s = \sigma + it $ with small $ \sigma $,
it looks like this integral will blowup. This is where Newman's key idea comes in to save us.

Newman proposes to multiply the integrand by $ (1 + \frac{s}{R^{2}}) e^{sT} $.
First of all, notice that adding $ \frac{s}{R^{2}}(F(s) - F_{T}(s)) $ doesn't change the integral
since the integrand is holomorphic. Multiplying the integrand by $ e^{sT} $ doesn't change anything either,
since $ e^{0} = 1 $ and $ e^{sT} $ is an entire function.

But now, we can get good upper bounds!

Let $ s = \sigma + it  \in \gamma_{R}^{+} $.

First of all, notice that 

\[ \frac{1}{s} + \frac{s}{R^{2}} = \frac{\bar{z}}{z^{2}} + \frac{z}{R^{2}} = \frac{2 \sigma}{R^{2}} \] 

Then, we have that

\begin{align*}
    \lvert (F(s) - F_{T}(s)) e^{Ts} (\frac{1}{s} + \frac{s}{R^{2}}) \rvert 
    = \lvert \int_{T}^{\infty} (f(t) e^{-st} dt) e^{Ts} \frac{2 \sigma}{R^{2}} \rvert \\
    = \frac{M}{\sigma} e^{-T\sigma} e{T \sigma}  \frac{2 \sigma}{R^{2}} = \frac{2M}{R^{2}}
\end{align*}

Thus, 

\[ \lvert \frac{1}{2 \pi i} \int_{\gamma_{R}^{+}} (F(s) - F_{T}(s)) e^{Ts} (\frac{1}{s} + \frac{s}{R^{2}}) ds \rvert \leq \frac{2M}{R} \]

Let's now bound $ \gamma_{R}^{-} $. 

First, notice that we can use the triangle inequality to separate the problem into bounding $ \lvert I_{1}(R) \rvert $ and
$ \lvert I_{2}(R) \rvert $, which are defined as

\[ I_{1}(R) = \frac{1}{2 \pi i} \int_{\gamma_{R}^{-}} F(s) e^{Ts} (\frac{1}{s} + \frac{s}{R^{2}}) ds \]

and

\[ I_{2}(R) = \frac{1}{2 \pi i} \int_{\gamma_{R}^{-}} F_{T}(s) e^{Ts} (\frac{1}{s} + \frac{s}{R^{2}}) ds \]

$ I_{1}(R) $ will be harder since we don't know what $ F(s) $ in this side of the plane. Thus, let's first
handle $ I_{2}(R) $.

Since 

\[ \lvert F_{T}(s) \rvert \leq M \int_{0}^{T} \lvert e^{-st} \rvert dt \leq M \frac{e^{-T \sigma}}{\sigma} \],

we have that the same bound as $ \gamma_{R}^{+} $ goes through. More specifically,

\begin{align*}
    \lvert \frac{1}{2 \pi i} \int_{\gamma_{R}^{-}} F_{T}(s) e^{Ts} (\frac{1}{s} + \frac{s}{R^{2}}) ds \rvert \leq 
    MR \frac{e^{-T \sigma}}{\sigma} e^{T \sigma} \frac{2\sigma}{R^{2}} = \frac{2M}{R}
\end{align*}

Now, we handle $ I_{1}(R) $ using the following strategy.
First, let $ B(R) $ be such that $ \lvert F(s) \rvert \leq B(R) $
on $ \gamma_{R}^{-} $.

We pick some $ \delta_{1} > 0 $ and divide the integral into two parts,
one with $ Re(s) \geq - \delta_{1} $ and the other with $ Re(s) < - \delta_{1} $.
In the latter, we can bound $ \sigma $ safely away from $ 0 $, and we'll suffocate
the former by taking $ \delta_{1} $ to $ 0 $, which is sufficient since 
the length of the integral is bounded above by $ 2 \arcsin \frac{\delta_{1}}{R} $.

More specifically, the latter portion is bounded above (for fixed $ R $ and $ \delta_{1} $) by

\begin{align}\label{latter}
    \frac{1}{2\pi} B(R) e^{-T \delta_{1}} (\frac{1}{\delta_{R}} + \frac{1}{R}) \pi R
\end{align}

Notice that letting $ T \to \infty $ would make this approach $ 0 $.

The former portion is bounded by

\begin{align}\label{former}
    \frac{1}{2\pi} B(R) (\frac{1}{\delta_{R}} + \frac{1}{R}) 2 R \arcsin \frac{\delta_{1}}{R}
\end{align}

But now we can make $ \delta_{1} $ as small as we want to take this expression to $ 0 $.

Let's now carefully start taking limits in a reasonable order to finish the proof.

Given some $ \epsilon > 0 $, take $ R $ to be large enough to make the
contribution by $ \gamma_{R}^{+} $ less than $ \epsilon $ and the contribution
by $ F_{T}(s) $ on $ \gamma_{R}^{-} $ also less than $ \epsilon $.

Then, pick $ \delta_{1} > 0 $ small enough to make \ref{former} less than $ \epsilon $.
Lastly, take $ T \to \infty $ to make \ref{latter} less than $ \epsilon $.
Thus, the entire difference is less than $ 4 \epsilon $ and we're done.


\section{Relationship of $ \Phi(s) $ to $ \frac{\zeta(s)^{\prime}}{\zeta(s)}$}

\begin{lemma}\label{lem:newman_v}

\end{lemma}



\begin{lemma}\label{lem:newman_v_conditions}
    $ \frac{\Phi(s + 1)}{s + 1} - \frac{1}{s} $ is analytic on 
    $ Re(s) \geq 0 $ if and only if $ \zeta(s) $ has no zeros
    with $ Re(s) = 1 $.
\end{lemma}
\begin{proof}
    Let's first prove the statement without the denominator.

    Notice that


    
    More specifically, let $ x = e^{t} $
    and notice that 
    
    $ f(x) = \frac{\nu(x)}{x} - 1 $, so $ f(x) \leq cx - 1 $ for some $ c > 0 $.
\end{proof}

\begin{lemma}\label{lem:epsilon_convergence_trick_phi}
    For any $ s $ with $ Re(s) > 1/2 $, the infinite series given by 
    \[ \sum_{p} \log(p)[\frac{1}{p^{s}(p^{s} - 1)}] \] 
    is convergent.
\end{lemma}
\begin{proof}
    
    Let $ \epsilon > 0 $ sufficiently small.

    For sufficiently large $ x > 0 $ we have that, $ x^{2s} - x^{s} \geq x^{2s - \epsilon} $.
    Then, we have that
    
    \[ \frac{1}{x^{2s} - x^{s}} \leq \frac{1}{x^{2s - \epsilon}} \]

    Then, we can use the comparison test to conclude the proof.
\end{proof}

\end{document}
